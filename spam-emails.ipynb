{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\ndf = pd.read_csv(\"../input/spam_ham_dataset.csv\")\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(df[\"text\"],df[\"label\"], test_size = 0.1, random_state = 10)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(X_train)\nX_train_df = vect.transform(X_train)\nX_test_df = vect.transform(X_test)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = RandomForestClassifier()\nmodel.fit(X_train_df,y_train)\n\ntarget = model.predict(X_test_df)\naccuracy_score(y_test,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1792fbb1fef96f17582d604524bf2cb7728edc1"},"cell_type":"code","source":"from joblib import dump, load\ndump(model, 'model.joblib')\ndump(vect, 'vector.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b406179f79cf683b03b8f252f40abaf3224674ff"},"cell_type":"code","source":"model = load('model.joblib')\nvect = load('vector.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9753bd483d2964680f8241668c1027dc4f133625"},"cell_type":"code","source":"def is_spam(inp = [\"FREE FREE FREE FREE\"]):\n    if model.predict(vect.transform(inp))[0] == \"spam\":\n        return True\n    else:\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a24118d1d2d0ef299079521ce258cb9296a33c7"},"cell_type":"code","source":"print(is_spam(inp = [\\\n                     \"\"\"Online Social Media platforms, such as Facebook and Twitter, enable all users, independently of their characteristics, to freely generate and consume huge amounts of data. While this data is being exploited by individuals and organisations to gain competitive advantage, a substantial amount of data is being generated by spam or fake users. One in every 200 social media messages and one in every 21 tweets is estimated to be spam. The rapid growth in the volume of global spam is expected to compromise research works that use social media data, thereby questioning data credibility. Motivated by the need to identify and filter out spam contents in social media data, this study presents a novel approach for distinguishing spam vs. non-spam social media posts and offers more insight into the behaviour of spam users on Twitter. The approach proposes an optimised set of features independent of historical tweets, which are only available for a short time on Twitter. We take into account features related to the users of Twitter, their accounts and their pairwise engagement with each other. We experimentally demonstrate the efficacy and robustness of our approach and compare it to a typical feature set for spam detection in the literature, achieving a significant improvement on performance. In contrast to prior research findings, we observe that an average automated spam account posted at least 12 tweets per day at well defined periods. Our method is suitable for real-time deployment in a social media data collection pipeline as an initial preprocessing strategy to improve the validity of research data.\"\"\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"654a20bd4a181434fa69e48984e6e0eac0298951"},"cell_type":"code","source":"print(is_spam(inp = [\"\"\"\\\nCongratulations You have won 10000$. FREE FREE FREE .Come and collect.\n\"\"\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6869cbc098fb490f0e10f0c74e141e504c59b04a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}